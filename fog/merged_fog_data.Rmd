Pr  rocess for creating `merged_fog_data.sas7bdat`

- Scan files for fog using Perl code (`create_fog_data.sh`, which runs `parse_xml_calls.pl`)
- Import fog data using `import_fog_speaker_data.pl`
- Re-arrange and merge fog data: `fog/cast_fog_data.R`
- Upload Stata file `~/Dropbox/research/BuGT/data/merged_fog_data.dta` to ACCT
- Run `import_stata.sas` to produce `merged_fog_data.sas7bdat`


- Run `get_fog_data.R` to produce `bgt.fog`

```
system("psql -f fog/fog_decomposed.sql")
```
- Run `fog_decomposed.sql`, which combines `bgt.fog` and `bgt.jargon_words` to produce `bgt.fog_decomposed`.
- Run `cast_fog_data.R` to produce `bgt.fog_recast`.
- Run `transfer_to_acct.R` to produce `fog_data.sas7bdat` and `fog_data_ticker.sas7bdat`.
- Run `data_test.R` to compare old and new data sets.

### Forward-looking sentence data

- 

### Tone data

- `get_lm_tone_words.R`: R code to get data from Bill McDonald's website and put it the `bgt.lm_tone` table.
- `tone_count.sql`: Creates PL/Python function to get tone words. Depends on `bgt.lm_tone` table.
- `get_tone_data.R`: Calculates tone variables for earnings conference calls and puts data in `bgt.tone_data`. 

### Jargon word data
`top_words.sql`: Code to create PL/Python functions `word_counts` and `top_words`.
`num_jargon_words.sql`

